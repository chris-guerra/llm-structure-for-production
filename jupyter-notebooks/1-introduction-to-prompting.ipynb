{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompting\n",
    "\n",
    "\"Prompting\" is the way in which humans communicate with an AI using human language to instruct the LLM systems. \n",
    "\n",
    "Prompt engineering is a discipline that creates and optimizes prompts to leverage language models. A prompt engineer translates an idea from conversational language to more precised and optimized instructions to an AI.\n",
    "\n",
    "In this Jupyter Notebook we provide a complete guide to prompting.\n",
    "\n",
    "# Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Prompting Techniques](#prompting-techniques)\n",
    "\n",
    "# Introduction\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "For a new project we import OpenAI from openai. This will automatically defaults to getting the key using **<u>os.environ.get(\"OPENAI_API_KEY\")</u>**, it's important that for this to automatically work your key must be named OPENAI_API_KEY. \n",
    "\n",
    "**Note:** If the code doesn't work, view Readme.md on how to set up this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your first prompt\n",
    "\n",
    "In this example we have a few lines of code:\n",
    "- prompt_system: we explain the system what role we expect it to take\n",
    "- prompt: we give instructions on what we want to do\n",
    "- response: we connect to OpenAIs API and give the model instructions\n",
    "\n",
    "### Example: Story teller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embark on a quest to find a legendary cheese that could grant him unlimited wisdom. With determination in his heart and a backpack full of supplies, Benjamin set off into the unknown, ready to face any challenges that lay ahead.\n"
     ]
    }
   ],
   "source": [
    "prompt_system = \"You are a helpful assistant whose goal is to help write stories.\"\n",
    "prompt = \"\"\"Continue the following story. Write no more than 50 words.\n",
    "\n",
    "Once upon a time, in a world where animals could speak, a courageous mouse named\n",
    "Benjamin decided to\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Techniques\n",
    "<a id=\"prompting-techniques\"></a>\n",
    "\n",
    "## Zero Shot Prompting\n",
    "\n",
    "**Zero Shot Prompting** is when a model is asked to produce output with our examples demonstrating the task. We tested it in the previous example. Here is another example.\n",
    "\n",
    "This example has a variable ***{topic}***. This would be an input from a program that someone can add of a poem writer assistant. We will add the variable at the beggining of the code and you can edit it from there. In future notebooks we will learn how to create our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the depths where whispers hide,\n",
      "Lies a essence, pure and wide.\n",
      "A soul that blooms like a flower bold,\n",
      "Guiding us through life's unfold.\n",
      "\n",
      "It dances with the rhythm of the heart,\n",
      "Weaving tales of worlds apart.\n",
      "A light that shines through every night,\n",
      "A guiding star, a beacon bright.\n"
     ]
    }
   ],
   "source": [
    "topic = \"soul\"\n",
    "# This is the prompt, we use the topic variable that will be our input somewhere where we implement our model\n",
    "#\n",
    "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
    "prompt = f\"\"\"Write a short poem about {topic}.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning and Few-Shot Prompting\n",
    "<a id=\"context-few-shot\"></a>\n",
    "\n",
    "**In context learning** is an approach where a model learns from examples or demonstrations in the prompt.\n",
    "\n",
    "**Few-Shot prompting** is a subset of in context learning that presents the model with a small set of relevant examples. This is useful for more complex tasks.\n",
    "\n",
    "Unlike zero shot, we use examples to improve performance.\n",
    "\n",
    "### In context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is a whisper in the wind,\n",
      "A melody that never ends.\n",
      "It warms the heart and soothes the soul,\n",
      "A treasure that makes us whole.\n"
     ]
    }
   ],
   "source": [
    "topic = \"love\"\n",
    "\n",
    "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
    "prompt = f\"\"\"Write a short poem about {topic}.\"\"\"\n",
    "examples = {\"nature\": \"\"\"Birdsong fills the air,\\nMountains high and valleys deep,\\nNature's music sweet.\"\"\", \n",
    "            \"winter\": \"\"\"Snow blankets the ground,\\nSilence is the only sound,\\nWinter's beauty found.\"\"\"\n",
    "            }\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot prompting\n",
    "\n",
    "For this example that is more complex we will use LangChain, this is a library designed to facilitate use of LLMs. We will go further into the topic in a future notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: purple\n",
      "Emotion: royalty\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"\"\"Here are some examples of colors and the emotions associated with \\\n",
    "them:\\n\\n\"\"\",\n",
    "    suffix=\"\"\"\\n\\nNow, given a new color, identify the emotion associated with \\\n",
    "it:\\n\\nColor: {input}\\nEmotion:\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt,\n",
    "input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input\n",
    "# color\n",
    "response = chain.run({})\n",
    "\n",
    "print(\"Color: purple\")\n",
    "print(\"Emotion:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
