{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompting\n",
    "\n",
    "\"Prompting\" is the way in which humans communicate with an AI using human language to instruct the LLM systems. \n",
    "\n",
    "Prompt engineering is a discipline that creates and optimizes prompts to leverage language models. A prompt engineer translates an idea from conversational language to more precised and optimized instructions to an AI.\n",
    "\n",
    "In this Jupyter Notebook we provide a complete guide to prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- Introduction\n",
    "- Creating your first prompt\n",
    "- Prompting Techniques\n",
    "    - Zero Shot Prompting\n",
    "    - In-Context Learning and Few-Shot Prompting\n",
    "        - In-Context Learning\n",
    "        - Few-Shot Prompting\n",
    "    - Role Prompting\n",
    "    - Chain Prompting\n",
    "    - Chain of Thought Prompting\n",
    "- Bad Prompting Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new project we import OpenAI from openai. This will automatically defaults to getting the key using **<u>os.environ.get(\"OPENAI_API_KEY\")</u>**, it's important that for this to automatically work your key must be named OPENAI_API_KEY. \n",
    "\n",
    "**Note:** If the code doesn't work, view Readme.md on how to set up this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your first prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have a few lines of code:\n",
    "- prompt_system: we explain the system what role we expect it to take\n",
    "- prompt: we give instructions on what we want to do\n",
    "- response: we connect to OpenAIs API and give the model instructions\n",
    "\n",
    "### Example: Story teller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embark on a dangerous journey to find the legendary Cheese Mountain. With his tiny walking stick in hand, he faced countless challenges, including cunning cats and treacherous traps. But Benjamin pressed on, determined to uncover the secret of the elusive mountain.\n"
     ]
    }
   ],
   "source": [
    "prompt_system = \"You are a helpful assistant whose goal is to help write stories.\"\n",
    "prompt = \"\"\"Continue the following story. Write no more than 50 words.\n",
    "\n",
    "Once upon a time, in a world where animals could speak, a courageous mouse named\n",
    "Benjamin decided to\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero Shot Prompting** is when a model is asked to produce output with our examples demonstrating the task. We tested it in the previous example. Here is another example.\n",
    "\n",
    "This example has a variable ***{topic}***. This would be an input from a program that someone can add of a poem writer assistant. We will add the variable at the beggining of the code and you can edit it from there. In future notebooks we will learn how to create our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the depths of the heart, a spark ignites,  \n",
      "A precious essence, a guiding light,  \n",
      "The soul that dances, pure and true,  \n",
      "Connecting us to all that's new.  \n",
      "So cherish your soul, let it shine bright,  \n",
      "For it's a beacon in the darkest night.\n"
     ]
    }
   ],
   "source": [
    "topic = \"soul\"\n",
    "# This is the prompt, we use the topic variable that will be our input somewhere where we implement our model\n",
    "#\n",
    "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
    "prompt = f\"\"\"Write a short poem about {topic}.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Context Learning and Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In context learning** is an approach where a model learns from examples or demonstrations in the prompt.\n",
    "\n",
    "**Few-Shot prompting** is a subset of in context learning that presents the model with a small set of relevant examples. This is useful for more complex tasks.\n",
    "\n",
    "Unlike zero shot, we use examples to improve performance.\n",
    "\n",
    "### In-Context learning\n",
    "\n",
    "As you can see in this example we provide three examples with three verses, if you see the output it follows the same structure, guiding itself from examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\n",
      "In the heart's quiet space,\n",
      "The soul finds its resting place,\n",
      "Guiding with gentle grace.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_topic = \"love\"\n",
    "\n",
    "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
    "prompt = f\"\"\"Write a short poem about {topic}.\"\"\"\n",
    "examples = {\"nature\": \"\"\"Birdsong fills the air,\\nMountains high and valleys deep,\\nNature's music sweet.\"\"\", \n",
    "            \"winter\": \"\"\"Snow blankets the ground,\\nSilence is the only sound,\\nWinter's beauty found.\"\"\"\n",
    "            }\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(topic=\"nature\")},\n",
    "        {\"role\": \"assistant\", \"content\": examples[\"nature\"]},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(topic=\"winter\")},\n",
    "        {\"role\": \"assistant\", \"content\": examples[\"winter\"]},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(topic=input_topic)}\n",
    "        ])\n",
    "\n",
    "print(f\"Output:\\n\\n{response.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot prompting\n",
    "\n",
    "For this example that is more complex we will use LangChain, this is a library designed to facilitate use of LLMs. We will go further into the topic in a future notebook.\n",
    "\n",
    "You can see the printed prompt to get an idea of how it works by putting together all this code into a final prompt.\n",
    "\n",
    "**Note:** While few-shot learning is effective, it encounters challenges, mainly when tasks are complex. More advanced strategies, like chain-of-thought prompting, become increasingly valuable in such cases. This technique breaks down complex problems into simpler phases, offering examples for each stage and enhancing the model’s logical reasoning capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Color: purple'\n",
      "Output: 'Emotion: royalty'\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Here are some examples of colors and the emotions associated with them:\n",
      "\n",
      "Color: red\n",
      "Emotion: passion\n",
      "\n",
      "\n",
      "Color: blue\n",
      "Emotion: serenity\n",
      "\n",
      "\n",
      "Color: green\n",
      "Emotion: tranquility\n",
      "\n",
      "Now, given a new color, identify the emotion associated with it:\n",
      "Color: purple\n",
      "Emotion:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"\"\"Here are some examples of colors and the emotions associated with \\\n",
    "them:\"\"\",\n",
    "    suffix=\"\"\"Now, given a new color, identify the emotion associated with \\\n",
    "it:\\nColor: {input}\\nEmotion:\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt,\n",
    "input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input\n",
    "# color\n",
    "response = chain.run({})\n",
    "\n",
    "# We print the example of the prompt sent\n",
    "print(\"Input: 'Color: purple'\")\n",
    "print(\"Output: 'Emotion:\", f\"{response}'\")\n",
    "print(f\"\\nPrompt:\\n\\n{chain.prompt.template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Role prompting involves instructing the LLM to assume a specific role or identity for task execution, such as functioning as a copywriter. This instruction can influence the model’s response by providing context or perspective for the task. When working with role prompts, the iterative process includes:\n",
    "\n",
    "1. Defining the role in the prompt. For example, “As a copywriter, create engaging catchphrases for AWS services.”\n",
    "2. Utilizing the prompt to generate a response from an LLM.\n",
    "3. Evaluating the response and refining the prompt as needed for improved outcomes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: interstellar travel\n",
      "Year: 3030\n",
      "AI-generated song title: \"Galactic Odyssey: Journey Through the Stars in 3030\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
    "What's a cool song title for a song about {theme} in the year {year}?\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"theme\", \"year\"],\n",
    "    template=template,\n",
    "    )\n",
    "\n",
    "# Input data for the prompt\n",
    "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
    "\n",
    "# Create LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the LLMChain to get the AI-generated song title\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Theme: interstellar travel\")\n",
    "print(\"Year: 3030\")\n",
    "print(\"AI-generated song title:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain Prompting involves linking a series of prompts sequentially, where the output from one prompt serves as the input for the next. When implementing chain prompting with LangChain, consider the following steps:\n",
    "\n",
    "- Identify and extract relevant information from the generated response.\n",
    "- Develop a new prompt using this extracted information, ensuring it builds upon the previous response.\n",
    "- Continue this process as necessary to reach the intended result.\n",
    "\n",
    "PromptTemplate class is designed to simplify the creation of prompts with dynamic inputs. This feature is particularly useful in constructing a prompt chain that relies on responses from previous prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist: Albert Einstein\n",
      "Fact: Albert Einstein's theory of general relativity, published in 1915, describes gravity as a curvature in the fabric of spacetime caused by the presence of mass and energy. According to this theory, massive objects like planets and stars warp the fabric of spacetime, causing other objects to move along curved paths. General relativity also predicts phenomena such as gravitational time dilation, gravitational waves, and the bending of light around massive objects.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Prompt 1\n",
    "template_question = \"\"\"\n",
    "What is the name of the famous scientist who developed the theory of general relativity?\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
    "\n",
    "# Prompt 2\n",
    "template_fact = \"\"\"\n",
    "Provide a brief description of {scientist}'s theory of general relativity.\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)\n",
    "\n",
    "# Create the LLMChain for the first prompt\n",
    "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "response_question = chain_question.run({})\n",
    "\n",
    "# Extract the scientist's name from the response\n",
    "\n",
    "scientist = response_question.strip()\n",
    "\n",
    "# Create the LLMChain for the second prompt\n",
    "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
    "\n",
    "# Input data for the second prompt\n",
    "input_data = {\"scientist\": scientist}\n",
    "\n",
    "# Run the LLMChain for the second prompt\n",
    "response_fact = chain_fact.run(input_data)\n",
    "\n",
    "print(\"Scientist:\", scientist)\n",
    "print(\"Fact:\", response_fact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Thought Prompting (CoT) is a method designed to prompt Large Language Models to articulate their thought process, enhancing the accuracy of the results. This technique involves presenting examples that showcase the reasoning process, guiding the LLM to explain its logic while responding to prompts. CoT has proven beneficial for arithmetic, common-sense reasoning, and symbolic thinking tasks.\n",
    "\n",
    "In the context of LangChain, CoT offers several advantages. Firstly, it simplifies complex tasks by enabling the LLM to break down challenging problems into more manageable steps. This feature is valuable for tasks requiring calculations, logical analysis, or multi-step reasoning. Secondly, CoT can guide the model through a series of related prompts, fostering more coherent and contextually appropriate outputs. This can result in more precise and practical responses, especially in tasks requiring a thorough understanding of the problem or subject matter.\n",
    "\n",
    "However, there are limitations to CoT that should be considered. One limitation is that it is effective primarily with models with around 100 billion parameters or more. Smaller models often produce nonsensical thought processes, reducing accuracy compared to traditional prompting methods. Another limitation is that CoT’s effectiveness varies across different types of tasks. While it shows significant benefits for tasks involving arithmetic, common sense, and symbolic reasoning, its impact on other tasks might be less meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips for Effective Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering is an iterative process, often requiring multiple adjustments to obtain the most accurate response. As LLMs continue integrating into various products and services, proficiency in devising effective prompts will become crucial. Here are the general rules to follow:\n",
    "\n",
    "- Be specific with your prompt: Include sufficient context and detail to guide the LLM toward the intended output.\n",
    "- Force conciseness when necessary.\n",
    "- Encourage the model to describe why it is the way it is: This can result in more precise solutions, particularly for complex tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
