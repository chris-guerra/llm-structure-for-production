{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompting\n",
    "\n",
    "\"Prompting\" is the way in which humans communicate with an AI using human language to instruct the LLM systems. \n",
    "\n",
    "Prompt engineering is a discipline that creates and optimizes prompts to leverage language models. A prompt engineer translates an idea from conversational language to more precised and optimized instructions to an AI.\n",
    "\n",
    "In this Jupyter Notebook we provide a complete guide to prompting.\n",
    "\n",
    "# Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Creating your first prompt](#first-prompt)\n",
    "- [Prompting Techniques](#prompting-techniques)\n",
    "    - [Zero Shot Prompting](#zero-shot-prompts)\n",
    "    - [In-Context Learning and Few-Shot Prompting](#context-few-shot)\n",
    "        - [In-Context Learning](#context-prompting)\n",
    "        - [Few-Shot Prompting](#few-shot-prompting)\n",
    "\n",
    "# Introduction\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "For a new project we import OpenAI from openai. This will automatically defaults to getting the key using **<u>os.environ.get(\"OPENAI_API_KEY\")</u>**, it's important that for this to automatically work your key must be named OPENAI_API_KEY. \n",
    "\n",
    "**Note:** If the code doesn't work, view Readme.md on how to set up this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your first prompt\n",
    "<a id=\"first-prompt\"></a>\n",
    "\n",
    "In this example we have a few lines of code:\n",
    "- prompt_system: we explain the system what role we expect it to take\n",
    "- prompt: we give instructions on what we want to do\n",
    "- response: we connect to OpenAIs API and give the model instructions\n",
    "\n",
    "### Example: Story teller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embark on a dangerous journey to find the legendary Cheese Mountain. With his tiny walking stick in hand, he faced countless challenges, including cunning cats and treacherous traps. But Benjamin pressed on, determined to uncover the secret of the elusive mountain.\n"
     ]
    }
   ],
   "source": [
    "prompt_system = \"You are a helpful assistant whose goal is to help write stories.\"\n",
    "prompt = \"\"\"Continue the following story. Write no more than 50 words.\n",
    "\n",
    "Once upon a time, in a world where animals could speak, a courageous mouse named\n",
    "Benjamin decided to\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Techniques\n",
    "<a id=\"prompting-techniques\"></a>\n",
    "\n",
    "### Zero Shot Prompting\n",
    "<a id=\"zero-shot-prompts\"></a>\n",
    "\n",
    "**Zero Shot Prompting** is when a model is asked to produce output with our examples demonstrating the task. We tested it in the previous example. Here is another example.\n",
    "\n",
    "This example has a variable ***{topic}***. This would be an input from a program that someone can add of a poem writer assistant. We will add the variable at the beggining of the code and you can edit it from there. In future notebooks we will learn how to create our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the depths of the heart, a spark ignites,  \n",
      "A precious essence, a guiding light,  \n",
      "The soul that dances, pure and true,  \n",
      "Connecting us to all that's new.  \n",
      "So cherish your soul, let it shine bright,  \n",
      "For it's a beacon in the darkest night.\n"
     ]
    }
   ],
   "source": [
    "topic = \"soul\"\n",
    "# This is the prompt, we use the topic variable that will be our input somewhere where we implement our model\n",
    "#\n",
    "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
    "prompt = f\"\"\"Write a short poem about {topic}.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning and Few-Shot Prompting\n",
    "<a id=\"context-few-shot\"></a>\n",
    "\n",
    "**In context learning** is an approach where a model learns from examples or demonstrations in the prompt.\n",
    "\n",
    "**Few-Shot prompting** is a subset of in context learning that presents the model with a small set of relevant examples. This is useful for more complex tasks.\n",
    "\n",
    "Unlike zero shot, we use examples to improve performance.\n",
    "\n",
    "## In-Context learning\n",
    "<a id=\"context-prompting\"></a>\n",
    "\n",
    "As you can see in this example we provide three examples with three verses, if you see the output it follows the same structure, guiding itself from examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\n",
      "In the heart's quiet space,\n",
      "The soul finds its resting place,\n",
      "Guiding with gentle grace.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_topic = \"love\"\n",
    "\n",
    "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
    "prompt = f\"\"\"Write a short poem about {topic}.\"\"\"\n",
    "examples = {\"nature\": \"\"\"Birdsong fills the air,\\nMountains high and valleys deep,\\nNature's music sweet.\"\"\", \n",
    "            \"winter\": \"\"\"Snow blankets the ground,\\nSilence is the only sound,\\nWinter's beauty found.\"\"\"\n",
    "            }\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_system},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(topic=\"nature\")},\n",
    "        {\"role\": \"assistant\", \"content\": examples[\"nature\"]},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(topic=\"winter\")},\n",
    "        {\"role\": \"assistant\", \"content\": examples[\"winter\"]},\n",
    "        {\"role\": \"user\", \"content\": prompt.format(topic=input_topic)}\n",
    "        ])\n",
    "\n",
    "print(f\"Output:\\n\\n{response.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot prompting\n",
    "<a id=\"few-shot-prompting\"></a>\n",
    "\n",
    "For this example that is more complex we will use LangChain, this is a library designed to facilitate use of LLMs. We will go further into the topic in a future notebook.\n",
    "\n",
    "You can see the printed prompt to get an idea of how it works by putting together all this code into a final prompt.\n",
    "\n",
    "**Note:** While few-shot learning is effective, it encounters challenges, mainly when tasks are complex. More advanced strategies, like chain-of-thought prompting, become increasingly valuable in such cases. This technique breaks down complex problems into simpler phases, offering examples for each stage and enhancing the model’s logical reasoning capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Color: purple'\n",
      "Output: 'Emotion: royalty'\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Here are some examples of colors and the emotions associated with them:\n",
      "\n",
      "Color: red\n",
      "Emotion: passion\n",
      "\n",
      "\n",
      "Color: blue\n",
      "Emotion: serenity\n",
      "\n",
      "\n",
      "Color: green\n",
      "Emotion: tranquility\n",
      "\n",
      "Now, given a new color, identify the emotion associated with it:\n",
      "Color: purple\n",
      "Emotion:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "examples = [\n",
    "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
    "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
    "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"\n",
    "Color: {color}\n",
    "Emotion: {emotion}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"color\", \"emotion\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"\"\"Here are some examples of colors and the emotions associated with \\\n",
    "them:\"\"\",\n",
    "    suffix=\"\"\"Now, given a new color, identify the emotion associated with \\\n",
    "it:\\nColor: {input}\\nEmotion:\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt,\n",
    "input_variables=[]))\n",
    "\n",
    "# Run the LLMChain to get the AI-generated emotion associated with the input\n",
    "# color\n",
    "response = chain.run({})\n",
    "\n",
    "# We print the example of the prompt sent\n",
    "print(\"Input: 'Color: purple'\")\n",
    "print(\"Output: 'Emotion:\", f\"{response}'\")\n",
    "print(f\"\\nPrompt:\\n\\n{chain.prompt.template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role Prompting\n",
    "\n",
    "Role prompting involves instructing the LLM to assume a specific role or identity for task execution, such as functioning as a copywriter. This instruction can influence the model’s response by providing context or perspective for the task. When working with role prompts, the iterative process includes:\n",
    "\n",
    "1. Defining the role in the prompt. For example, “As a copywriter, create engaging catchphrases for AWS services.”\n",
    "2. Utilizing the prompt to generate a response from an LLM.\n",
    "3. Evaluating the response and refining the prompt as needed for improved outcomes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: interstellar travel\n",
      "Year: 3030\n",
      "AI-generated song title: \"Galactic Odyssey: Journey Through the Stars in 3030\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
    "What's a cool song title for a song about {theme} in the year {year}?\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"theme\", \"year\"],\n",
    "    template=template,\n",
    "    )\n",
    "\n",
    "# Input data for the prompt\n",
    "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
    "\n",
    "# Create LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the LLMChain to get the AI-generated song title\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Theme: interstellar travel\")\n",
    "print(\"Year: 3030\")\n",
    "print(\"AI-generated song title:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes a good prompt?\n",
    "\n",
    "**Precise Directions:** The prompt is structured as a straightforward request for generating a song title, explicitly stating the context: “As a futuristic robot band conductor.” This clarity helps the LLM recognize that the output should be a song title linked to a futuristic context.\n",
    "\n",
    "**Specificity:** The prompt calls for a song title connected to a particular theme and year, “{theme} in the year {year}.” This level of detail allows the LLM to produce a relevant and imaginative response. The flexibility of the prompt to accommodate various themes and years through input variables adds to its versatility and applicability.\n",
    "\n",
    "**Promoting Creativity:** The prompt does not restrict the LLM to a specific format or style for the song title, encouraging a wide range of creative responses based on the specified theme and year.\n",
    "\n",
    "**Concentrated on the Task:** The prompt concentrates exclusively on creating a song title, simplifying the LLM process to deliver an appropriate response without being diverted by unrelated subjects. Integrating multiple tasks in one prompt can confuse the model, potentially compromising its effectiveness in each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
