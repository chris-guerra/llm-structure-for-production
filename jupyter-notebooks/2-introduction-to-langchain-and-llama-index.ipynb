{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and Llama-Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- LangChain Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is an open-source framework designed to simplify the development, productionization, and deployment of applications powered by Large Language Models (LLMs). It provides a set of building blocks, components, and integrations that simplify every stage of the LLM application lifecycle.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "- Abstractions and **LangChain Expression Language (LCEL)** for composing chains.\n",
    "- **Third-party integrations** and partner packages for easy extensibility.\n",
    "- **Chains, agents, and retrieval strategies** for building cognitive architectures.\n",
    "- **LangGraph**: for creating robust, stateful multi-actor applications.\n",
    "- **LangServe**: for deploying LangChain chains as REST APIs.\n",
    "\n",
    "The broader LangChain ecosystem also includes **LangSmith**, a developer platform for debugging, testing, evaluating, and monitoring LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChains role in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval-augmented generation (RAG)** is a useful technique for addressing one of the main challenges associated with Large Language Models (LLMs): hallucinations. By integrating external knowledge sources, RAG systems can provide LLMs with relevant, factual information during the generation process. This ensures that the generated outputs are more accurate, reliable, and contextually appropriate. \n",
    "\n",
    "LangChain provides useful abstractions for building RAG systems. With LangChain’s retrieval components, developers can easily integrate external data sources, such as documents or databases, into their LLM-powered applications. This allows the models to access and utilize relevant information during the generation process, enabling more accurate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Langchain concepts and Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompts: LangChain provides tooling to create and work with prompt templates. Prompt templates are predefined recipes for generating prompts for language models.\n",
    "- Output Parsers: Output parsers are classes that help structure language model responses. They are responsible for taking the output of an LLM and transforming it into a more suitable format.\n",
    "- Retrievers: Retrievers accept a string query as input and return a list of Documents as output. LangChain provides several advanced retrieval types and also integrates with many third-party retrieval services.\n",
    "- Document Loaders: A Document is a piece of text and associated metadata. Document loaders provide a “load” method for loading data as documents from a configured source.\n",
    "- Text Splitters: Text splitters divide a document or text into smaller chunks or segments. LangChain has a number of built-in document transformers that can split, combine, and filter documents.\n",
    "- Indexes: An index in LangChain is a data structure that organizes and stores data to facilitate quick and efficient searches.\n",
    "- Embeddings models: The Embeddings class is designed to interface with text embedding models. It provides a standard interface for different embedding model providers, such as OpenAI, Cohere, Hugging Face, etc.\n",
    "- Vector Stores: A vector store stores embedded data and performs vector search. Embedding and storing embedding vectors is one of the most common ways to store and search over unstructured data.\n",
    "- Agents: Agents are the decision-making components that decide the plan of action or process.\n",
    "- Chains: They are sequences of calls, whether to an LLM, a tool, or a data preprocessing step. They integrate various components into a user-friendly interface, including the model, prompt, memory, output parsing, and debugging capabilities.\n",
    "- Tool: A tool is a specific function that helps the language model gather the necessary information for task completion. Tools can range from Google Searches and database queries to Python REPL and other chains.\n",
    "- Memory: This feature records past interactions with a language model, providing context for future interactions.\n",
    "- Callbacks: LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, and streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents & Tools Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Agents**\n",
    "LangChain agents complete tasks using chains, prompts, memory, and tools. These agents can perform diverse tasks, including executing steps in a predetermined sequence, interfacing with external systems such as Gmail or SQL databases, and more. LangChain offers a range of tools and features to support the customization of agents for various applications.\n",
    "\n",
    "\n",
    "Agent Types\n",
    "LangChain has a variety of agent types, each with its specialized functions.\n",
    "- Zero-shot ReAct: This agent uses the ReAct framework to decide tool usage based on the descriptions. It’s termed “zero-shot” because it relies only on the tool descriptions without the need for specific usage examples.\n",
    "- Structured Input ReAct: This agent manages tools that necessitate multiple inputs.\n",
    "• OpenAI Functions Agent: This agent is specifically developed for function calls for fine-tuned models and is compatible with advanced models such as gpt-3.5-turbo and gpt-4-turbo.\n",
    "- Self-Ask with Search Agent: This agent sources factual responses to questions, specializing in the “Intermediate Answer” tool. It is similar to the methodology in the original self-ask with search research.\n",
    "- ReAct Document Store Agent: This agent combines the “Search” and “Lookup” tools to provide a continuous thought process.\n",
    "- Plan-and-Execute Agents: This type formulates a plan consisting of multiple actions, which are then carried out sequentially. These agents are particularly effective for complex or long-running tasks, maintaining a steady focus on long-term goals. However, one trade-off of using these agents is the potential for increased latency.\n",
    "\n",
    "The agents essentially determine the logic behind selecting an action and deciding whether to use multiple tools, a single tool or none, based on the task.\n",
    "\n",
    "\n",
    "Available Tools and Custom Tools\n",
    "A list of tools that integrate LangChain with other tools is accessible at Toolkits section the LangChain docs. Some examples are:\n",
    "\n",
    "- The Python tool: It’s used to generate and execute Python codes to answer a question.\n",
    "- The JSON tool: It’s used when interacting with a JSON file that doesn’t fit in the LLM context window.\n",
    "- The CSV tool: It’s used to interact with CSV files.\n",
    "\n",
    "The degree of customization is dependent on the development of advanced interactions. In such cases, tools can be coordinated to execute complex behaviors. Examples include generating questions, conducting web searches for answers, and compiling summaries of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM Powered Aplications with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides standard tools for interacting with LLMs. The ChatPromptTemplate is used for structuring conversations with AI models, aiding in controlling the conversation’s flow and content. LangChain employs message prompt templates to construct and work with prompts, maximizing the potential of the underlying chat model.\n",
    "\n",
    "Different types of prompts serve varied purposes in interactions with chat models. \n",
    "\n",
    "The SystemMessagePromptTemplate provides initial instructions, context, or data for the AI model. \n",
    "\n",
    "In contrast, HumanMessagePromptTemplate consists of user messages that the AI model answers.\n",
    "\n",
    " To demonstrate, we will create a chat-based assistant for movie information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Inception\" is a 2010 science fiction action film written and directed by Christopher Nolan. The film stars Leonardo DiCaprio as a professional thief who steals information by entering the subconscious minds of his targets through their dreams. The plot follows his final job of planting an idea into the mind of a CEO by navigating through multiple layers of dreams.\n",
      "\n",
      "The movie explores themes of reality, dreams, and the power of the mind. It received critical acclaim for its originality, visual effects, and performances. \"Inception\" was a commercial success and won four Academy Awards for Best Cinematography, Best Sound Editing, Best Sound Mixing, and Best Visual Effects.\n",
      "\n",
      "If you are interested in watching a mind-bending and visually stunning film, \"Inception\" is highly recommended.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define the system and human message templates\n",
    "template = \"You are an assistant that helps users find information about movies.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Find information about the movie {movie_title}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create the chat prompt template from system and human message prompts\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Format the prompt with the movie title\n",
    "formatted_prompt = chat_prompt.format_prompt(movie_title=\"Inception\").to_messages()\n",
    "\n",
    "# Use the invoke method to get the response\n",
    "response = chat.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The to_messages object in LangChain is a practical tool for converting the formatted value of a chat prompt template into a list of message objects. This functionality proves particularly beneficial when working with chat models, providing a structured method to oversee the conversation. This ensures that the chat model effectively comprehends the context and roles of the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarization Chain Example\n",
    "\n",
    "A summarization chain interacts with external data sources to retrieve information for use in the generation phase. This process may involve condensing extensive text or using specific data sources to answer questions.\n",
    "\n",
    "To initiate this process, the language model is configured using the OpenAI class with a temperature setting 0, for a fully deterministic output. \n",
    "\n",
    "The load_summarize_chain function takes an instance of the language model and sets up a pre-built summarization chain. \n",
    "\n",
    "Furthermore, the PyPDFLoader class loads PDF files and transforms them into a format that LangChain can process efficiently. It’s essential to have the pypdf package installed to execute the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El informe trimestral del proyecto Passerelles de USAID, presentado por Eliane Kouton da Conceicao el 30 de julio de 2020, describe la respuesta del proyecto a la pandemia de COVID-19 en Senegal desde abril hasta junio de 2020. Las actividades clave incluyeron la distribución de información sobre salud, la realización de transmisiones radiales y el envío de mensajes SMS para promover la concienciación sobre la salud y la continuidad educativa para niños de 9 a 16 años en las regiones de Casamance y Kédougou. El proyecto se adaptó a los desafíos de la pandemia desarrollando planes de contingencia, utilizando reuniones virtuales y creando una plataforma de formación a distancia para el aprendizaje socio-emocional. Involucró a las comunidades en los protocolos de seguridad y apoyó la educación de los niños vulnerables. La colaboración con el gobierno y organizaciones tuvo como objetivo fortalecer los servicios educativos. El proyecto identificó a 408 jóvenes con teléfonos inteligentes para una iniciativa de COVID-19 y enfrentó desafíos debido a las restricciones, pero logró que el 56% de los participantes continuaran su educación. En general, el proyecto se centró en la educación inclusiva y la resiliencia comunitaria en medio de la crisis sanitaria en curso.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Function to load text from PDF using PyPDFLoader\n",
    "def load_text_from_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Path to the PDF file\n",
    "file_path = \"/Users/christian_guerra/Desktop/PA00X8TQ.pdf\"\n",
    "\n",
    "# Load text from PDF\n",
    "documents = load_text_from_pdf(file_path)\n",
    "\n",
    "# Choose summarization chain type: \"stuff\", \"map_reduce\", or \"refine\"\n",
    "chain_type = \"map_reduce\"  # or \"stuff\" or \"refine\"\n",
    "initial_summary_chain = load_summarize_chain(llm, chain_type=chain_type)\n",
    "\n",
    "# Summarize the document\n",
    "initial_summary = initial_summary_chain.invoke(documents)\n",
    "\n",
    "# Translate\n",
    "translate_template = \"\"\"\n",
    "Translate the text wrapped in triple parenthesis to to {language}. ((({text})))\"\"\"\n",
    "translate_prompt = PromptTemplate(template=translate_template, input_variables=['language', 'text'])\n",
    "\n",
    "summary_question = LLMChain(prompt=translate_prompt, llm=llm)\n",
    "\n",
    "# Create the input dictionary with the expected keys\n",
    "input_data = {'language': 'Español', 'text': initial_summary[\"output_text\"]}\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "summary_response = summary_question.run(input_data)\n",
    "# Print the final summary\n",
    "print(summary_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El informe trimestral del proyecto USAID Passerelles, presentado por Eliane Kouton da Conceicao el 30 de julio de 2020, describe la respuesta del proyecto a la pandemia de COVID-19 en Senegal desde abril hasta junio de 2020. Las actividades clave incluyeron la distribución de información sobre salud, la realización de transmisiones por radio y el envío de mensajes SMS para promover la concienciación sobre la salud y la continuidad educativa para niños de 9 a 16 años en las regiones de Casamance y Kédougou. El proyecto se adaptó a los desafíos de la pandemia desarrollando planes de contingencia, utilizando reuniones virtuales y creando una plataforma de formación a distancia para el aprendizaje socio-emocional. Involucró a las comunidades en los protocolos de seguridad y apoyó la educación de los niños vulnerables. La colaboración con el gobierno y organizaciones buscó fortalecer los servicios educativos. El proyecto identificó a 408 jóvenes con teléfonos inteligentes para una iniciativa de COVID-19 y enfrentó desafíos debido a las restricciones, pero logró que el 56% de los participantes continuaran su educación. En general, el proyecto se centró en la educación inclusiva y la resiliencia comunitaria en medio de la crisis sanitaria en curso.\n"
     ]
    }
   ],
   "source": [
    "# Translate\n",
    "translate_template = \"\"\"\n",
    "Translate the text wrapped in triple parenthesis to to {language}. Make sure the answer is useful ((({text})))\"\"\"\n",
    "translate_prompt = PromptTemplate(template=translate_template, input_variables=['language', 'text'])\n",
    "\n",
    "summary_question = LLMChain(prompt=translate_prompt, llm=llm)\n",
    "\n",
    "# Create the input dictionary with the expected keys\n",
    "input_data = {'language': 'Español', 'text': initial_summary[\"output_text\"]}\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "summary_response = summary_question.run(input_data)\n",
    "# Print the final summary\n",
    "print(summary_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
