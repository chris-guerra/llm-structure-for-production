{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain and Llama-Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- LangChain Introduction\n",
    "- LangChain Agents & Tools Overview\n",
    "- Building LLM Powered Aplications with LangChain\n",
    "- LLama Index Introduction\n",
    "- LangChain vs. LlamaIndex vs. OpenAI Assistants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is an open-source framework designed to simplify the development, productionization, and deployment of applications powered by Large Language Models (LLMs). It provides a set of building blocks, components, and integrations that simplify every stage of the LLM application lifecycle.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "- Abstractions and **LangChain Expression Language (LCEL)** for composing chains.\n",
    "- **Third-party integrations** and partner packages for easy extensibility.\n",
    "- **Chains, agents, and retrieval strategies** for building cognitive architectures.\n",
    "- **LangGraph**: for creating robust, stateful multi-actor applications.\n",
    "- **LangServe**: for deploying LangChain chains as REST APIs.\n",
    "\n",
    "The broader LangChain ecosystem also includes **LangSmith**, a developer platform for debugging, testing, evaluating, and monitoring LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChains Role in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval-augmented generation (RAG)** is a useful technique for addressing one of the main challenges associated with Large Language Models (LLMs): hallucinations. By integrating external knowledge sources, RAG systems can provide LLMs with relevant, factual information during the generation process. This ensures that the generated outputs are more accurate, reliable, and contextually appropriate. \n",
    "\n",
    "LangChain provides useful abstractions for building RAG systems. With LangChain’s retrieval components, developers can easily integrate external data sources, such as documents or databases, into their LLM-powered applications. This allows the models to access and utilize relevant information during the generation process, enabling more accurate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key LangChain Concepts and Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompts: LangChain provides tooling to create and work with prompt templates. Prompt templates are predefined recipes for generating prompts for language models.\n",
    "- Output Parsers: Output parsers are classes that help structure language model responses. They are responsible for taking the output of an LLM and transforming it into a more suitable format.\n",
    "- Retrievers: Retrievers accept a string query as input and return a list of Documents as output. LangChain provides several advanced retrieval types and also integrates with many third-party retrieval services.\n",
    "- Document Loaders: A Document is a piece of text and associated metadata. Document loaders provide a “load” method for loading data as documents from a configured source.\n",
    "- Text Splitters: Text splitters divide a document or text into smaller chunks or segments. LangChain has a number of built-in document transformers that can split, combine, and filter documents.\n",
    "- Indexes: An index in LangChain is a data structure that organizes and stores data to facilitate quick and efficient searches.\n",
    "- Embeddings models: The Embeddings class is designed to interface with text embedding models. It provides a standard interface for different embedding model providers, such as OpenAI, Cohere, Hugging Face, etc.\n",
    "- Vector Stores: A vector store stores embedded data and performs vector search. Embedding and storing embedding vectors is one of the most common ways to store and search over unstructured data.\n",
    "- Agents: Agents are the decision-making components that decide the plan of action or process.\n",
    "- Chains: They are sequences of calls, whether to an LLM, a tool, or a data preprocessing step. They integrate various components into a user-friendly interface, including the model, prompt, memory, output parsing, and debugging capabilities.\n",
    "- Tool: A tool is a specific function that helps the language model gather the necessary information for task completion. Tools can range from Google Searches and database queries to Python REPL and other chains.\n",
    "- Memory: This feature records past interactions with a language model, providing context for future interactions.\n",
    "- Callbacks: LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, and streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents & Tools Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Agents**\n",
    "LangChain agents complete tasks using chains, prompts, memory, and tools. These agents can perform diverse tasks, including executing steps in a predetermined sequence, interfacing with external systems such as Gmail or SQL databases, and more. LangChain offers a range of tools and features to support the customization of agents for various applications.\n",
    "\n",
    "\n",
    "Agent Types\n",
    "LangChain has a variety of agent types, each with its specialized functions.\n",
    "- Zero-shot ReAct: This agent uses the ReAct framework to decide tool usage based on the descriptions. It’s termed “zero-shot” because it relies only on the tool descriptions without the need for specific usage examples.\n",
    "- Structured Input ReAct: This agent manages tools that necessitate multiple inputs.\n",
    "- OpenAI Functions Agent: This agent is specifically developed for function calls for fine-tuned models and is compatible with advanced models such as gpt-3.5-turbo and gpt-4-turbo.\n",
    "- Self-Ask with Search Agent: This agent sources factual responses to questions, specializing in the “Intermediate Answer” tool. It is similar to the methodology in the original self-ask with search research.\n",
    "- ReAct Document Store Agent: This agent combines the “Search” and “Lookup” tools to provide a continuous thought process.\n",
    "- Plan-and-Execute Agents: This type formulates a plan consisting of multiple actions, which are then carried out sequentially. These agents are particularly effective for complex or long-running tasks, maintaining a steady focus on long-term goals. However, one trade-off of using these agents is the potential for increased latency.\n",
    "\n",
    "The agents essentially determine the logic behind selecting an action and deciding whether to use multiple tools, a single tool or none, based on the task.\n",
    "\n",
    "\n",
    "Available Tools and Custom Tools\n",
    "A list of tools that integrate LangChain with other tools is accessible at Toolkits section the LangChain docs. Some examples are:\n",
    "\n",
    "- The Python tool: It’s used to generate and execute Python codes to answer a question.\n",
    "- The JSON tool: It’s used when interacting with a JSON file that doesn’t fit in the LLM context window.\n",
    "- The CSV tool: It’s used to interact with CSV files.\n",
    "\n",
    "The degree of customization is dependent on the development of advanced interactions. In such cases, tools can be coordinated to execute complex behaviors. Examples include generating questions, conducting web searches for answers, and compiling summaries of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLM Powered Aplications with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides standard tools for interacting with LLMs. The ChatPromptTemplate is used for structuring conversations with AI models, aiding in controlling the conversation’s flow and content. LangChain employs message prompt templates to construct and work with prompts, maximizing the potential of the underlying chat model.\n",
    "\n",
    "Different types of prompts serve varied purposes in interactions with chat models. \n",
    "\n",
    "The SystemMessagePromptTemplate provides initial instructions, context, or data for the AI model. \n",
    "\n",
    "In contrast, HumanMessagePromptTemplate consists of user messages that the AI model answers.\n",
    "\n",
    " To demonstrate, we will create a chat-based assistant for movie information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Inception\" is a science fiction film released in 2010, written and directed by Christopher Nolan. The film stars Leonardo DiCaprio as Dom Cobb, a skilled thief who specializes in the art of \"extraction,\" which involves stealing secrets from within the subconscious during the dream state. The plot revolves around Cobb being offered a chance to have his criminal history erased if he can successfully perform \"inception,\" the act of planting an idea into someone's mind without them realizing it.\n",
      "\n",
      "Key details about the film include:\n",
      "\n",
      "- **Cast**: In addition to Leonardo DiCaprio, the film features an ensemble cast that includes Joseph Gordon-Levitt, Ellen Page (now Elliot Page), Tom Hardy, Ken Watanabe, Cillian Murphy, Marion Cotillard, and Michael Caine.\n",
      "\n",
      "- **Themes**: \"Inception\" explores complex themes such as the nature of reality, dreams versus waking life, and the power of the subconscious. It raises questions about perception, memory, and the impact of ideas.\n",
      "\n",
      "- **Visual Effects**: The film is renowned for its groundbreaking visual effects, including the iconic scenes of bending cityscapes and gravity-defying action sequences.\n",
      "\n",
      "- **Music**: The score was composed by Hans Zimmer, and the film features the memorable track \"Time,\" which has become particularly popular.\n",
      "\n",
      "- **Awards**: \"Inception\" received critical acclaim and won four Academy Awards, including Best Cinematography, Best Visual Effects, Best Sound Editing, and Best Sound Mixing. It was also nominated for Best Picture and Best Original Screenplay.\n",
      "\n",
      "- **Box Office**: The film was a commercial success, grossing over $836 million worldwide.\n",
      "\n",
      "\"Inception\" is often regarded as one of the best films of the 21st century and has garnered a significant following for its intricate plot and innovative storytelling techniques.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "MODEL = os.getenv(\"OPENAI_CHAT_MODEL\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "chat = ChatOpenAI(model=MODEL, temperature=0)\n",
    "\n",
    "# Define the system and human message templates\n",
    "template = \"You are an assistant that helps users find information about movies.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Find information about the movie {movie_title}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create the chat prompt template from system and human message prompts\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Format the prompt with the movie title\n",
    "formatted_prompt = chat_prompt.format_prompt(movie_title=\"Inception\").to_messages()\n",
    "\n",
    "# Use the invoke method to get the response\n",
    "response = chat.invoke(formatted_prompt)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The to_messages object in LangChain is a practical tool for converting the formatted value of a chat prompt template into a list of message objects. This functionality proves particularly beneficial when working with chat models, providing a structured method to oversee the conversation. This ensures that the chat model effectively comprehends the context and roles of the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarization Chain Example\n",
    "\n",
    "A summarization chain interacts with external data sources to retrieve information for use in the generation phase. This process may involve condensing extensive text or using specific data sources to answer questions.\n",
    "\n",
    "To initiate this process, the language model is configured using the OpenAI class with a temperature setting 0, for a fully deterministic output. \n",
    "\n",
    "The load_summarize_chain function takes an instance of the language model and sets up a pre-built summarization chain. \n",
    "\n",
    "Furthermore, the PyPDFLoader class loads PDF files and transforms them into a format that LangChain can process efficiently. It’s essential to have the pypdf package installed to execute the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El informe trimestral del proyecto Passerelles de USAID, presentado por Eliane Kouton da Conceicao el 30 de julio de 2020, describe la respuesta del proyecto a la pandemia de COVID-19 en Senegal desde abril hasta junio de 2020. Las actividades clave incluyeron la distribución de información sobre salud, la realización de transmisiones radiales y el envío de mensajes SMS para promover la concienciación sobre la salud y la continuidad educativa para niños de 9 a 16 años en las regiones de Casamance y Kédougou. El proyecto se adaptó a los desafíos de la pandemia desarrollando planes de contingencia, utilizando reuniones virtuales y creando una plataforma de formación a distancia para el aprendizaje socio-emocional. Involucró a las comunidades en los protocolos de seguridad y apoyó la educación de los niños vulnerables. La colaboración con el gobierno y organizaciones tuvo como objetivo fortalecer los servicios educativos. El proyecto identificó a 408 jóvenes con teléfonos inteligentes para una iniciativa de COVID-19 y enfrentó desafíos debido a las restricciones, pero logró que el 56% de los participantes continuaran su educación. En general, el proyecto se centró en la educación inclusiva y la resiliencia comunitaria en medio de la crisis sanitaria en curso.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Function to load text from PDF using PyPDFLoader\n",
    "def load_text_from_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Path to the PDF file\n",
    "file_path = \"/Users/christian_guerra/Desktop/PA00X8TQ.pdf\"\n",
    "\n",
    "# Load text from PDF\n",
    "documents = load_text_from_pdf(file_path)\n",
    "\n",
    "# Choose summarization chain type: \"stuff\", \"map_reduce\", or \"refine\"\n",
    "chain_type = \"map_reduce\"  # or \"stuff\" or \"refine\"\n",
    "initial_summary_chain = load_summarize_chain(llm, chain_type=chain_type)\n",
    "\n",
    "# Summarize the document\n",
    "initial_summary = initial_summary_chain.invoke(documents)\n",
    "\n",
    "# Translate\n",
    "translate_template = \"\"\"\n",
    "Translate the text wrapped in triple parenthesis to to {language}. ((({text})))\"\"\"\n",
    "translate_prompt = PromptTemplate(template=translate_template, input_variables=['language', 'text'])\n",
    "\n",
    "summary_question = LLMChain(prompt=translate_prompt, llm=llm)\n",
    "\n",
    "# Create the input dictionary with the expected keys\n",
    "input_data = {'language': 'Español', 'text': initial_summary[\"output_text\"]}\n",
    "\n",
    "# Run the LLMChain for the first prompt with an empty dictionary\n",
    "summary_response = summary_question.run(input_data)\n",
    "# Print the final summary\n",
    "print(summary_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Chain Example\n",
    "\n",
    "LangChain can structure prompts in several ways, including asking general questions to language models. Be mindful of the potential for hallucinations and instances where the models might generate information that is not factual. We can implement a retrieval-augmented generation system to mitigate this problem.\n",
    "\n",
    "We establish a customized prompt template by initializing an instance of the PromptTemplate class. This template string incorporates a {question} placeholder for the input query, followed by a newline character and the “Answer:” tag. The input_variables parameter is assigned to a list of existing placeholders in the prompt (a question in this scenario) to represent the variable name, and they will be substituted by the input argument using the template’s .run() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a deeply philosophical question that has been debated for centuries. Different people and cultures have different beliefs about the purpose and meaning of life. Some believe that the meaning of life is to seek happiness and fulfillment, others believe it is to serve a higher power or fulfill a specific destiny, while others believe that life has no inherent meaning and it is up to each individual to create their own meaning. Ultimately, the meaning of life is a personal and subjective question that each person must answer for themselves.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Question: {question}\\nAnswer:\", \n",
    "    input_variables=[\"question\"])\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "answer = chain.run(\"what is the meaning of life?\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama Index Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex, like other LLM tooling frameworks, allows for the easy creation of LLM-powered apps with useful and straightforward abstractions. When we want to develop retrieval-augmented generation (RAG) systems, LlamaIndex makes it simple to combine extracting relevant information from large databases with the text generation capabilities of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector stores are databases that keep and manage embeddings, which are long lists of numbers representing input data’s meaning. Embeddings capture the essence of data, be it words, images, or anything else, depending on how the embedding model is made.\n",
    "\n",
    "Vector stores efficiently store, find, and study large amounts of complex data. By turning data into embeddings, vector stores enable searches based on meaning and similarity, which is better than just matching keywords.\n",
    "\n",
    "Embedding models are AI tools that learn to convert input data into vectors. The input data type depends on the specific use case and how the embedding model is designed. For example:\n",
    "\n",
    "- In text processing, embedding models can map words into vectors based on their use in a large text collection.\n",
    "- In computer vision, embedding models can map images into vectors that capture their visual features and meaning.\n",
    "- In recommendation systems, embedding models can represent users and items as vectors based on interactions and likes.\n",
    "\n",
    "Once the data is converted to embeddings, vector stores can quickly find similar items because similar things are represented by vectors close to each other in the vector space.\n",
    "\n",
    "Semantic search, which uses vector stores, understands the meaning of a query by comparing its embedding with the embeddings of the stored data. This ensures that the search results are relevant and match the intended meaning, no matter what specific words are used in the query or what type of data is being searched.\n",
    "\n",
    "Vector stores enable meaningful searches and similarity-based retrieval, making them a powerful tool for handling large, complex datasets in many AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep lake vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples throughout this book, we will use Deep Lake as our vector store database to demonstrate how to build and manage AI applications effectively. However, it’s important to note that multiple vector store databases are available, both open-source and managed options.\n",
    "\n",
    "The choice of which vector store to use depends on factors such as the AI application’s specific requirements, the level of support needed, and the budget available. It’s up to you to evaluate and choose the vector store that best suits your needs.\n",
    "\n",
    "Deep Lake is a vector store database designed to support AI applications, particularly those involving Large Language Models (LLMs) and deep learning. It provides a storage format optimized for storing various data types, including embeddings, audio, text, videos, images, PDFs, and annotations.\n",
    "\n",
    "Deep Lake offers features such as querying, vector search, data streaming for training models at scale, data versioning, and lineage. It integrates with tools like LangChain, LlamaIndex, Weights & Biases, and others, allowing developers to build and manage AI applications more effectively.\n",
    "\n",
    "Some of the core features of Deep Lake include:\n",
    "\n",
    "- Multi-cloud support: Deep Lake works with various cloud storage providers like S3, GCP, and Azure, as well as local and in-memory storage.\n",
    "- Native compression with lazy NumPy-like indexing: It allows data to be stored in their native compression formats and provides efficient slicing, indexing, and iteration over the data.\n",
    "- Dataset version control: Deep Lake brings concepts like commits, branches, and checkouts to dataset management, enabling better collaboration and reproducibility.\n",
    "- Built-in dataloaders for popular deep learning frameworks: It offers dataloaders for PyTorch and TensorFlow, facilitating the process of training models on large datasets.\n",
    "- Integrations with various tools: Deep Lake integrates with tools like LangChain and LlamaIndex for building LLM apps, Weights & Biases for data lineage during model training, and MMDetection for object detection tasks.\n",
    "\n",
    "By providing a range of features and integrations, Deep Lake aims to support the development and deployment of AI applications across a variety of use cases. While we will be using Deep Lake in our examples, the concepts and techniques discussed can be applied to other vector store databases as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of RAG-based applications is notably improved when they access a vector store compiling information from multiple sources. However, handling data in various formats presents particular challenges.\n",
    "\n",
    "Data connectors, known as Readers, play a crucial role. They parse and convert data into a more manageable Document format, which includes text and basic metadata, and simplify the data ingestion process. They automate data collection from different sources, including APIs, PDFs, and SQL databases, and effectively format this data.\n",
    "\n",
    "The open-source project LlamaHub hosts various data connectors to incorporate multiple data formats into the LLM.\n",
    "You can check out some of the loaders on the LlamaHub repository, where you can find various integrations and data sources. We will test the Wikipedia integration.\n",
    "\n",
    "LlamaIndex defaults to using OpenAI’s get-3.5-turbo for text generation and text-embedding-ada-002 model for embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LlamaIndex, the documents undergo a transformation within a processing framework after data ingestion. This process converts documents into smaller, more detailed units called Node objects. Nodes are derived from the original documents and include the primary content, metadata, and contextual details. LlamaIndex includes a NodeParser class, automatically transforming document content into structured nodes. We used SimpleNodeParser to turn a list of document objects into node objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex is proficient in indexing and searching through diverse data formats, including documents, PDFs, and database queries. Indexing represents a foundational step in data storage within a database. This process involves transforming unstructured data into embeddings that capture semantic meanings. This transformation optimizes the data format, facilitating easy access and querying.\n",
    "\n",
    "LlamaIndex offers various index types, each designed to fulfill a different purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Summary Index extracts a summary from each document and saves it with all its nodes. Having a document summary can be helpful, especially when matching small node embeddings with a query is not always straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vector Store Index generates embeddings during index construction to identify the top-k most similar nodes in response to a query.\n",
    "\n",
    "It’s suitable for small-scale applications and easily scalable to accommodate larger datasets using high-performance vector databases.\n",
    "\n",
    "For our example, we will save the crawled Wikipedia documents in a Deep Lake vector storage and build an index object based on their data. Using the DeepLakeVectorStore class, we will generate the dataset in Activeloop and attach documents to it. First, set the environment’s Activeloop and OpenAI API keys.\n",
    "\n",
    "Use the DeepLakeVectorStore class with the dataset_path as a parameter to connect to the platform. Replace the genai360 name with your organization ID (which defaults to your Activeloop account) to save the dataset to your workspace. The following code will generate an empty dataset.\n",
    "\n",
    "Establish a storage context using the StorageContext class and the Deep Lake dataset as the source. Pass this storage to a VectorStoreIndex class to generate the index (embeddings) and store the results on the specified dataset.\n",
    "\n",
    "The Deep Lake database efficiently stores and retrieves high-dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use the produced indexes to search through the data. The Query Engine is a pipeline that combines a Retriever and a Response Synthesizer. The pipeline retrieves nodes using the query string and then sends them to the LLM to build a response. A query engine can be constructed by invoking the as_query_engine() method on a previously created index.\n",
    "\n",
    "The following code uses documents from a Wikipedia page to build a Vector Store Index through the GPTVectorStoreIndex class. The .from_documents() method streamlines the process of constructing indexes from these processed documents. Once the index is created, it can be employed to create a query_engine object. This object enables asking questions about the documents using the .query() method.\n",
    "\n",
    "The indexes can also function solely as retrievers for fetching documents relevant to a query. This capability enables the creation of a Custom Query Engine, offering more control over various aspects, such as the prompt or the output format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routers help select the most suitable retriever for extracting context from a knowledge base. They choose the most appropriate query engine for a specific task, enhancing performance and accuracy.\n",
    "\n",
    "This functionality is particularly advantageous in scenarios involving multiple data sources, where each source contains distinct information. For instance, routers determine which data source is the most relevant for a given query in an application that uses a SQL database and a Vector Store as its knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Indexes Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All examples we looked at involved indexes stored on cloud-based vector stores like Deep Lake. However, in some cases, preserving the data on a disk may be necessary for speedy testing. “Storing” refers to saving index data, which comprises nodes and their embeddings, to disk. This is done by calling the persist() method on the storage_context object associated with the index:\n",
    "\n",
    "If the index is already in storage, you can load it instead of rebuilding it. Simply determine whether or not the index already exists on disk and continue accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain vs. LlamaIndex vs. OpenAI Assistants\n",
    "\n",
    "LangChain and LlamaIndex are tools that make developing applications with LLMs easier. \n",
    "\n",
    "Each offers distinct advantages:\n",
    "\n",
    "- **LangChain:** LangChain is designed for dynamic, context-rich interactions, making it highly suitable for applications such as chatbots and virtual assistants. Its strengths lie in its rapid prototyping capacity and application development ease.\n",
    "- **LlamaIndex:** LlamaIndex is proficient at processing, structuring, and accessing private or domain-specific data, targeting specific interactions with LLMs. It excels in high-precision and quality tasks, especially when handling specialized, domain-specific data. LlamaIndex’s primary strength is connecting LLMs with various data sources.\n",
    "-  **OpenAI’s Assistants** is another tool that makes building apps with Large Language Models (LLMs) easier, similar to LangChain and LlamaIndex. With this API, you can create AI assistants in your current apps using OpenAI LLMs. The Assistants API has three main features: a Code Interpreter to ****write and run Python code safely, Knowledge Retrieval to find information, and Function Calling to add your own functions or tools to the Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
